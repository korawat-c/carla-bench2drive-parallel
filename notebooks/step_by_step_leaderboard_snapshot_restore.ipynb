{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import argparse\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "import traceback\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import socket\n",
    "import atexit\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# os.environ['CUDA_HOME'] = '/usr/local/cuda-12.2'\n",
    "os.environ['IS_BENCH2DRIVE'] = 'true'\n",
    "os.environ['REPETITION'] = '1'\n",
    "# os.environ['DATAGEN'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing CarlaUE4 process with PID 2308331 on GPU 0\n",
      "No python processes found on GPU 0\n",
      "No CarlaUE4 processes found on GPU 1\n",
      "No python processes found on GPU 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess, re\n",
    "\n",
    "def kill_carla_processes_on_gpu(gpu_index: int = 0):\n",
    "    \"\"\"Kill CARLA processes on specific GPU\"\"\"\n",
    "    try:\n",
    "        raw = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "        pattern = rf'\\|\\s+{gpu_index}\\s+\\S+\\s+\\S+\\s+(\\d+)\\s+C\\+G\\s+.*CarlaUE4'\n",
    "        pids = re.findall(pattern, raw)\n",
    "        if not pids:\n",
    "            print(f\"No CarlaUE4 processes found on GPU {gpu_index}\")\n",
    "            return\n",
    "        for pid in pids:\n",
    "            print(f\"Killing CarlaUE4 process with PID {pid} on GPU {gpu_index}\")\n",
    "            subprocess.run([\"kill\", \"-9\", pid], check=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error killing CARLA processes: {e}\")\n",
    "\n",
    "def kill_python_processes_on_gpu(gpu_index: int = 0):\n",
    "    \"\"\"Kill Python processes on specific GPU\"\"\"\n",
    "    try:\n",
    "        raw = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "        pattern = rf'\\|\\s+{gpu_index}\\s+\\S+\\s+\\S+\\s+(\\d+)\\s+C\\s+.*bin/python'\n",
    "        pids = re.findall(pattern, raw)\n",
    "        if not pids:\n",
    "            print(f\"No python processes found on GPU {gpu_index}\")\n",
    "            return\n",
    "        for pid in pids:\n",
    "            print(f\"Killing python process with PID {pid} on GPU {gpu_index}\")\n",
    "            subprocess.run([\"kill\", \"-9\", pid], check=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error killing Python processes: {e}\")\n",
    "\n",
    "def cleanup_gpu_processes():\n",
    "    \"\"\"Clean up GPU processes\"\"\"\n",
    "    for gpu_id in [0, 1]:\n",
    "        kill_carla_processes_on_gpu(gpu_id)\n",
    "        kill_python_processes_on_gpu(gpu_id)\n",
    "\n",
    "cleanup_gpu_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARLA_ROOT:  /mnt3/Documents/AD_Framework/carla0915\n",
      "WORK_DIR:  /mnt3/Documents/AD_Framework/Bench2Drive\n",
      "SCENARIO_RUNNER_ROOT:  /mnt3/Documents/AD_Framework/Bench2Drive/scenario_runner\n",
      "LD_LIBRARY_PATH:  /usr/local/cuda-12.1/lib64:\n",
      "LEADERBOARD_ROOT:  /mnt3/Documents/AD_Framework/Bench2Drive/leaderboard\n",
      "CUDA_HOME:  /usr/local/cuda-12.1\n",
      "PYTHONPATH:  /mnt3/Documents/AD_Framework/carla0915/PythonAPI/carla/:/mnt3/Documents/AD_Framework/Bench2Drive/scenario_runner:/mnt3/Documents/AD_Framework/Bench2Drive/leaderboard::/mnt3/Documents/AD_Framework/Bench2Drive:/mnt3/Documents/AD_Framework/carla0915/PythonAPI:/mnt3/Documents/AD_Framework/carla0915/PythonAPI/carla:/mnt3/Documents/AD_Framework/carla0915/PythonAPI/carla/:/mnt3/Documents/AD_Framework/Bench2Drive/scenario_runner:/mnt3/Documents/AD_Framework/Bench2Drive/leaderboard::/mnt3/Documents/AD_Framework/Bench2Drive:/mnt3/Documents/AD_Framework/carla0915/PythonAPI:/mnt3/Documents/AD_Framework/carla0915/PythonAPI/carla\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "leaderboard_path = '/mnt3/Documents/AD_Framework/Bench2Drive/leaderboard/leaderboard'\n",
    "if leaderboard_path not in sys.path:\n",
    "    sys.path.insert(0, leaderboard_path)\n",
    "\n",
    "# Check environment variables\n",
    "print(\"CARLA_ROOT: \", os.environ.get('CARLA_ROOT'))\n",
    "print(\"WORK_DIR: \", os.environ.get('WORK_DIR'))\n",
    "print(\"SCENARIO_RUNNER_ROOT: \", os.environ.get('SCENARIO_RUNNER_ROOT'))\n",
    "print(\"LD_LIBRARY_PATH: \", os.environ.get('LD_LIBRARY_PATH'))\n",
    "print(\"LEADERBOARD_ROOT: \", os.environ.get('LEADERBOARD_ROOT'))\n",
    "print(\"CUDA_HOME: \", os.environ.get('CUDA_HOME'))\n",
    "print(\"PYTHONPATH: \", os.environ.get('PYTHONPATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.agent /mnt3/Documents/AD_Framework/Bench2Drive/leaderboard/leaderboard/autoagents/dummy_agent4.py\n",
      "SAVE_PATH set to: /mnt3/Documents/AD_Framework/results/my_model_mlp\n",
      "TMP_VISU set to: 0\n",
      "Track set to: SENSORS\n"
     ]
    }
   ],
   "source": [
    "port = 20019\n",
    "\n",
    "model_name = 'my_model_mlp'\n",
    "statistics_manager_path = f'/mnt3/Documents/AD_Framework/results/{model_name}'\n",
    "\n",
    "# makedir if do not exist\n",
    "os.makedirs(statistics_manager_path, exist_ok=True)\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.WORK_DIR = os.environ.get('WORK_DIR')\n",
    "args.checkpoint = f'{statistics_manager_path}/stat_manager_results.json'\n",
    "args.debug_checkpoint = f'{statistics_manager_path}/live_results.txt'\n",
    "args.host = '127.0.0.3'\n",
    "args.port = port\n",
    "args.timeout = 6000\n",
    "args.frame_rate = 20.0 # fixed\n",
    "args.gpu_rank = 0\n",
    "args.traffic_manager_port = port + 3\n",
    "# '/po3/korawat/Documents/AD_Framework/Bench2Drive/leaderboard/leaderboard/autoagents/dummy_agent.py'\n",
    "# args.TEAM_AGENT = f\"{args.WORK_DIR}/leaderboard/team_code/data_agent2.py\" # auto_pilot.py\n",
    "args.TEAM_AGENT = f'{leaderboard_path}/autoagents/dummy_agent4.py'\n",
    "## auto_pilot.py\n",
    "args.agent = args.TEAM_AGENT\n",
    "print(\"args.agent\", args.agent)\n",
    "args.agent_config = ''\n",
    "args.debug = 1\n",
    "args.routes = f\"/mnt3/Documents/AD_Framework/Bench2Drive/leaderboard/data/bench2drive220.xml\"\n",
    "args.repetitions = 1\n",
    "args.track = 'SENSORS' # Changed from 'MAP' to 'SENSORS' to allow camera sensors\n",
    "args.record = ''\n",
    "args.routes_subset = 0\n",
    "args.resume = 0\n",
    "args.traffic_manager_seed = port - 3333 + 1\n",
    "\n",
    "# Enable camera sensors by setting required environment variables\n",
    "os.environ['SAVE_PATH'] = statistics_manager_path\n",
    "os.environ['IS_BENCH2DRIVE'] = 'true'\n",
    "os.environ['TMP_VISU'] = '0'  # Enable camera sensors for visualization\n",
    "\n",
    "print(f\"SAVE_PATH set to: {statistics_manager_path}\")\n",
    "print(f\"TMP_VISU set to: {os.environ.get('TMP_VISU')}\")\n",
    "print(f\"Track set to: {args.track}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srunner.scenariomanager.carla_data_provider import *\n",
    "from srunner.scenariomanager.timer import GameTime\n",
    "from srunner.scenariomanager.watchdog import Watchdog\n",
    "\n",
    "from leaderboard.scenarios.scenario_manager import ScenarioManager\n",
    "from leaderboard.scenarios.route_scenario import RouteScenario\n",
    "from leaderboard.envs.sensor_interface import SensorConfigurationInvalid\n",
    "from leaderboard.autoagents.agent_wrapper import AgentError, validate_sensor_configuration, TickRuntimeError\n",
    "from leaderboard.utils.statistics_manager import StatisticsManager, FAILURE_MESSAGES\n",
    "from leaderboard.utils.route_indexer import RouteIndexer\n",
    "\n",
    "# print(\"args: \", args)\n",
    "statistics_manager = StatisticsManager(args.checkpoint , args.debug_checkpoint)\n",
    "# statistics_manager.write_statistics() ## run one time\n",
    "\n",
    "# leaderboard_evaluator = LeaderboardEvaluator(args, statistics_manager)\n",
    "\n",
    "LeaderboardEvaluator_path = '/po3/korawat/Documents/AD_Framework/Bench2Drive/leaderboard/leaderboard/leaderboard_evaluator.py'\n",
    "if LeaderboardEvaluator_path not in sys.path:\n",
    "    sys.path.insert(0, LeaderboardEvaluator_path)\n",
    "\n",
    "from leaderboard.leaderboard_evaluator import LeaderboardEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import subprocess\n",
    "import atexit\n",
    "\n",
    "\n",
    "def find_free_port(starting_port):\n",
    "    \"\"\"Find a free port starting from the given port\"\"\"\n",
    "    port = starting_port\n",
    "    while True:\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind((\"localhost\", port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            port += 1\n",
    "\n",
    "def _setup_simulation(self, args):\n",
    "    \"\"\"Setup simulation with CARLA server\"\"\"\n",
    "    os.system(\"pkill -f CarlaUE4\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    self.carla_path = os.environ[\"CARLA_ROOT\"]\n",
    "    args.port = find_free_port(args.port)\n",
    "    cmd1 = f\"{os.path.join(self.carla_path, 'CarlaUE4.sh')} -RenderOffScreen -nosound -quality-level=Epic -carla-rpc-port={args.port} -carla-streaming-port=0 -graphicsadapter=0\"\n",
    "    self.server = subprocess.Popen(cmd1, shell=True, preexec_fn=os.setsid)\n",
    "    atexit.register(os.killpg, self.server.pid, signal.SIGKILL)\n",
    "\n",
    "    # Wait for server to be ready\n",
    "    server_ready = False\n",
    "    for _ in range(30):\n",
    "        try:\n",
    "            with socket.create_connection(('localhost', args.port), timeout=1):\n",
    "                server_ready = True\n",
    "                break\n",
    "        except (ConnectionRefusedError, socket.timeout):\n",
    "            time.sleep(5)\n",
    "    if not server_ready:\n",
    "        raise RuntimeError(\"Carla server failed to start\")\n",
    "        \n",
    "    attempts = 0\n",
    "    num_max_restarts = 20\n",
    "    print(\"Loading world!!!\")\n",
    "    while attempts < num_max_restarts:\n",
    "        try:\n",
    "            client = carla.Client(args.host, args.port)\n",
    "            if args.timeout:\n",
    "                client_timeout = args.timeout\n",
    "            client.set_timeout(client_timeout)\n",
    "\n",
    "            settings = carla.WorldSettings(\n",
    "                synchronous_mode = True,\n",
    "                fixed_delta_seconds = 1.0 / self.frame_rate,\n",
    "                deterministic_ragdolls = True,\n",
    "                spectator_as_ego = False\n",
    "            )\n",
    "            client.get_world().apply_settings(settings)\n",
    "            print(f\"load_world success , attempts={attempts}\", flush=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"load_world failed , attempts={attempts}\", flush=True)\n",
    "            print(e, flush=True)\n",
    "            attempts += 1\n",
    "            time.sleep(5)\n",
    "    \n",
    "    attempts = 0\n",
    "    num_max_restarts = 40\n",
    "    while attempts < num_max_restarts:\n",
    "        try:\n",
    "            args.traffic_manager_port = find_free_port(args.traffic_manager_port)\n",
    "            traffic_manager = client.get_trafficmanager(args.traffic_manager_port)\n",
    "            traffic_manager.set_synchronous_mode(True)\n",
    "            traffic_manager.set_hybrid_physics_mode(True)\n",
    "            print(f\"traffic_manager init success, try_time={attempts}\", flush=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"traffic_manager init fail, try_time={attempts}\", flush=True)\n",
    "            print(e, flush=True)\n",
    "            attempts += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    return client, client_timeout, traffic_manager\n",
    "\n",
    "\n",
    "def get_weather_id(weather_conditions):\n",
    "    from xml.etree import ElementTree as ET\n",
    "    WORK_DIR = os.environ.get(\"WORK_DIR\", \"\")\n",
    "    if WORK_DIR != \"\":\n",
    "        WORK_DIR += \"/\"\n",
    "    tree = ET.parse(WORK_DIR + 'leaderboard/data/weather.xml')\n",
    "    root = tree.getroot()\n",
    "    def conditions_match(weather, conditions):\n",
    "        for (key, value) in weather:\n",
    "            if key == 'route_percentage' : continue\n",
    "            if str(getattr(conditions, key))!= value:\n",
    "                return False\n",
    "        return True\n",
    "    for case in root.findall('case'):\n",
    "        weather = case[0].items()\n",
    "        if conditions_match(weather, weather_conditions):\n",
    "            return case.items()[0][1]\n",
    "    return None\n",
    "\n",
    "sensors_to_icons = {\n",
    "    'sensor.camera.rgb':        'carla_camera',\n",
    "    'sensor.lidar.ray_cast':    'carla_lidar',\n",
    "    'sensor.other.radar':       'carla_radar',\n",
    "    'sensor.other.gnss':        'carla_gnss',\n",
    "    'sensor.other.imu':         'carla_imu',\n",
    "    'sensor.opendrive_map':     'carla_opendrive_map',\n",
    "    'sensor.speedometer':       'carla_speedometer'\n",
    "}\n",
    "\n",
    "#### the following are the functions that equal to _load_and_run_scenario in leaderboard_evaluator.py\n",
    "\n",
    "def my_load_scenario(leaderboard_evaluator_self, args, config):\n",
    "    \"\"\"Load and run the scenario given by config\"\"\"\n",
    "    crash_message = \"\"\n",
    "    entry_status = \"Started\"\n",
    "    save_name = \"\"\n",
    "\n",
    "    print(\"\\n\\033[1m========= Preparing {} (repetition {}) =========\\033[0m\".format(config.name, config.repetition_index), flush=True)\n",
    "\n",
    "    # Prepare the statistics of the route\n",
    "    route_name = f\"{config.name}_rep{config.repetition_index}\"\n",
    "    scenario_name = config.scenario_configs[0].name\n",
    "    town_name = str(config.town)\n",
    "    weather_id = get_weather_id(config.weather[0][1])\n",
    "    currentDateAndTime = datetime.now()\n",
    "    currentTime = currentDateAndTime.strftime(\"%m_%d_%H_%M_%S\")\n",
    "    save_name = f\"{route_name}_{town_name}_{scenario_name}_{weather_id}_{currentTime}\"\n",
    "    leaderboard_evaluator_self.statistics_manager.create_route_data(route_name, scenario_name, weather_id, save_name, town_name, config.index)\n",
    "\n",
    "    print(\"\\033[1m> Loading the world\\033[0m\", flush=True)\n",
    "\n",
    "    # Load the world and the scenario\n",
    "    try:\n",
    "        leaderboard_evaluator_self._load_and_wait_for_world(args, config.town)\n",
    "        from leaderboard.scenarios.route_scenario import RouteScenario\n",
    "        leaderboard_evaluator_self.route_scenario = RouteScenario(world=leaderboard_evaluator_self.world, config=config, debug_mode=args.debug)\n",
    "        leaderboard_evaluator_self.statistics_manager.set_scenario(leaderboard_evaluator_self.route_scenario)\n",
    "\n",
    "    except Exception:\n",
    "        # The scenario is wrong -> set the ejecution to crashed and stop\n",
    "        print(\"\\n\\033[91mThe scenario could not be loaded:\", flush=True)\n",
    "        print(f\"\\n{traceback.format_exc()}\\033[0m\", flush=True)\n",
    "\n",
    "        from leaderboard.utils.statistics_manager import FAILURE_MESSAGES\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Simulation\"]\n",
    "        leaderboard_evaluator_self._register_statistics(config.index, entry_status, crash_message)\n",
    "        leaderboard_evaluator_self._cleanup()\n",
    "        return True, save_name, entry_status, crash_message\n",
    "\n",
    "    return False, save_name, entry_status, crash_message\n",
    "\n",
    "def my_load_agent(leaderboard_evaluator_self, args, config, save_name, entry_status, crash_message):\n",
    "    \"\"\"Set up the user's agent and configure sensors\"\"\"\n",
    "    print(\"\\033[1m> Setting up the agent\\033[0m\", flush=True)\n",
    "\n",
    "    # Set up the user's agent, and the timer to avoid freezing the simulation\n",
    "    try:\n",
    "        from srunner.scenariomanager.watchdog import Watchdog\n",
    "        from leaderboard.envs.sensor_interface import SensorConfigurationInvalid\n",
    "        from leaderboard.autoagents.agent_wrapper import validate_sensor_configuration\n",
    "        from leaderboard.utils.statistics_manager import FAILURE_MESSAGES\n",
    "        \n",
    "        leaderboard_evaluator_self._agent_watchdog = Watchdog(args.timeout)\n",
    "        leaderboard_evaluator_self._agent_watchdog.start()\n",
    "        agent_class_name = getattr(leaderboard_evaluator_self.module_agent, 'get_entry_point')()\n",
    "        agent_class_obj = getattr(leaderboard_evaluator_self.module_agent, agent_class_name)\n",
    "\n",
    "        # Start the ROS1 bridge server only for ROS1 based agents.\n",
    "        if getattr(agent_class_obj, 'get_ros_version')() == 1 and leaderboard_evaluator_self._ros1_server is None:\n",
    "            from leaderboard.autoagents.ros1_agent import ROS1Server\n",
    "            leaderboard_evaluator_self._ros1_server = ROS1Server()\n",
    "            leaderboard_evaluator_self._ros1_server.start()\n",
    "\n",
    "        leaderboard_evaluator_self.agent_instance = agent_class_obj(args.host, args.port, args.debug)\n",
    "        leaderboard_evaluator_self.agent_instance.set_global_plan(leaderboard_evaluator_self.route_scenario.gps_route, leaderboard_evaluator_self.route_scenario.route)\n",
    "        args.agent_config = args.agent_config + '+' + save_name\n",
    "        leaderboard_evaluator_self.agent_instance.setup(args.agent_config)\n",
    "\n",
    "        # Check and store the sensors\n",
    "        if not leaderboard_evaluator_self.sensors:\n",
    "            leaderboard_evaluator_self.sensors = leaderboard_evaluator_self.agent_instance.sensors()\n",
    "            track = leaderboard_evaluator_self.agent_instance.track\n",
    "\n",
    "            validate_sensor_configuration(leaderboard_evaluator_self.sensors, track, args.track)\n",
    "\n",
    "            leaderboard_evaluator_self.sensor_icons = [sensors_to_icons[sensor['type']] for sensor in leaderboard_evaluator_self.sensors]\n",
    "            leaderboard_evaluator_self.statistics_manager.save_sensors(leaderboard_evaluator_self.sensor_icons)\n",
    "            leaderboard_evaluator_self.statistics_manager.write_statistics()\n",
    "\n",
    "            leaderboard_evaluator_self.sensors_initialized = True\n",
    "\n",
    "        leaderboard_evaluator_self._agent_watchdog.stop()\n",
    "        leaderboard_evaluator_self._agent_watchdog = None\n",
    "\n",
    "    except SensorConfigurationInvalid as e:\n",
    "        # The sensors are invalid -> set the ejecution to rejected and stop\n",
    "        print(\"\\n\\033[91mThe sensor's configuration used is invalid:\", flush=True)\n",
    "        print(f\"{e}\\033[0m\\n\", flush=True)\n",
    "\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Sensors\"]\n",
    "        leaderboard_evaluator_self._register_statistics(config.index, entry_status, crash_message)\n",
    "        leaderboard_evaluator_self._cleanup()\n",
    "        return True, entry_status, crash_message\n",
    "\n",
    "    except Exception as e:\n",
    "        # The agent setup has failed -> start the next route\n",
    "        print(\"\\n\\033[91mCould not set up the required agent:\", flush=True)\n",
    "        print(f\"\\n{traceback.format_exc()}\\033[0m\", flush=True)\n",
    "        print(f\"{e}\\033[0m\\n\", flush=True)\n",
    "\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Agent_init\"]\n",
    "        leaderboard_evaluator_self._register_statistics(config.index, entry_status, crash_message)\n",
    "        leaderboard_evaluator_self._cleanup()\n",
    "        return True, entry_status, crash_message\n",
    "\n",
    "    return False, entry_status, crash_message\n",
    "\n",
    "def my_load_route_scenario(leaderboard_evaluator_self, args, config, entry_status, crash_message):\n",
    "    \"\"\"Load the route scenario and prepare for execution\"\"\"\n",
    "    from leaderboard.utils.statistics_manager import FAILURE_MESSAGES\n",
    "    \n",
    "    print(\"\\033[1m> Running the route\\033[0m\", flush=True)\n",
    "\n",
    "    # Run the scenario\n",
    "    try:\n",
    "        # Load scenario and run it\n",
    "        if args.record:\n",
    "            leaderboard_evaluator_self.client.start_recorder(\"{}/{}_rep{}.log\".format(args.record, config.name, config.repetition_index))\n",
    "        leaderboard_evaluator_self.manager.load_scenario(leaderboard_evaluator_self.route_scenario, leaderboard_evaluator_self.agent_instance, config.index, config.repetition_index)\n",
    "    except Exception:\n",
    "        print(\"\\n\\033[91mFailed to load the scenario, the statistics might be empty:\", flush=True)\n",
    "        print(\"\\n\\033[91mLoading the route, the agent has crashed:\", flush=True)\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Agent_runtime\"]\n",
    "\n",
    "    return False, entry_status, crash_message\n",
    "\n",
    "def my_run_scenario_setup(leaderboard_evaluator_self):\n",
    "    \"\"\"Trigger the start of the scenario and wait for it to finish/fail\"\"\"\n",
    "    from srunner.scenariomanager.timer import GameTime\n",
    "    from srunner.scenariomanager.watchdog import Watchdog\n",
    "    \n",
    "    leaderboard_evaluator_self.manager.tick_count = 0\n",
    "    leaderboard_evaluator_self.manager.start_system_time = time.time()\n",
    "    leaderboard_evaluator_self.manager.start_game_time = GameTime.get_time()\n",
    "\n",
    "    # Detects if the simulation is down\n",
    "    leaderboard_evaluator_self.manager._watchdog = Watchdog(leaderboard_evaluator_self.manager._timeout)\n",
    "    leaderboard_evaluator_self.manager._watchdog.start()\n",
    "\n",
    "    # Stop the agent from freezing the simulation\n",
    "    leaderboard_evaluator_self.manager._agent_watchdog = Watchdog(leaderboard_evaluator_self.manager._timeout)\n",
    "    leaderboard_evaluator_self.manager._agent_watchdog.start()\n",
    "\n",
    "    leaderboard_evaluator_self.manager._running = True\n",
    "\n",
    "    # Thread for build_scenarios\n",
    "    leaderboard_evaluator_self.manager._scenario_thread = threading.Thread(target=leaderboard_evaluator_self.manager.build_scenarios_loop, args=(leaderboard_evaluator_self.manager._debug_mode > 0, ))\n",
    "    leaderboard_evaluator_self.manager._scenario_thread.start()\n",
    "\n",
    "def my_run_scenario_step(leaderboard_evaluator_self, entry_status, crash_message, n_steps=1):\n",
    "    \"\"\"Run n_steps of the scenario simulation\"\"\"\n",
    "    from leaderboard.autoagents.agent_wrapper import AgentError, TickRuntimeError\n",
    "    from leaderboard.utils.statistics_manager import FAILURE_MESSAGES\n",
    "    \n",
    "    try:\n",
    "        if leaderboard_evaluator_self.manager._running:\n",
    "            print(\"In my_run_scenario_step (Check if it is stuck or not)\")\n",
    "            for _ in range(n_steps):\n",
    "                leaderboard_evaluator_self.manager._tick_scenario()\n",
    "\n",
    "    except AgentError:\n",
    "        # The agent has failed -> stop the route\n",
    "        print(\"\\n\\033[91mStopping the route, the agent has crashed:\", flush=True)\n",
    "        print(f\"\\n{traceback.format_exc()}\\033[0m\")\n",
    "\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Agent_runtime\"]\n",
    "\n",
    "        return True, entry_status, crash_message\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return True, entry_status, crash_message\n",
    "    \n",
    "    except TickRuntimeError:\n",
    "        entry_status, crash_message = \"Started\", \"TickRuntime\"\n",
    "        return True, entry_status, crash_message\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\n\\033[91mError during the simulation:\", flush=True)\n",
    "        print(f\"\\n{traceback.format_exc()}\\033[0m\", flush=True)\n",
    "\n",
    "        entry_status, crash_message = FAILURE_MESSAGES[\"Simulation\"]\n",
    "        return True, entry_status, crash_message\n",
    "    \n",
    "    return False, entry_status, crash_message\n",
    "\n",
    "def my_stop_scenario(leaderboard_evaluator_self, args, config, entry_status, crash_message):\n",
    "    \"\"\"Stop the scenario and register statistics\"\"\"\n",
    "    from leaderboard.utils.statistics_manager import FAILURE_MESSAGES\n",
    "    \n",
    "    # Stop the scenario\n",
    "    try:\n",
    "        print(\"\\033[1m> Stopping the route\\033[0m\", flush=True)\n",
    "        leaderboard_evaluator_self.manager.stop_scenario()\n",
    "        leaderboard_evaluator_self._register_statistics(config.index, entry_status, crash_message)\n",
    "\n",
    "        if args.record:\n",
    "            leaderboard_evaluator_self.client.stop_recorder()\n",
    "\n",
    "        leaderboard_evaluator_self._cleanup()\n",
    "\n",
    "    except Exception:\n",
    "        print(\"\\n\\033[91mFailed to stop the scenario, the statistics might be empty:\", flush=True)\n",
    "        print(f\"\\n{traceback.format_exc()}\\033[0m\", flush=True)\n",
    "\n",
    "        _, crash_message = FAILURE_MESSAGES[\"Simulation\"]\n",
    "\n",
    "    return False, entry_status, crash_message\n",
    "\n",
    "# replace _setup_simulation method of LeaderboardEvaluator with this new one \n",
    "LeaderboardEvaluator._setup_simulation = _setup_simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.2-0+++UE4+Release-4.26 522 0\n",
      "Disabling core dumps.\n",
      "Loading world!!!\n",
      "load_world success , attempts=0\n",
      "traffic_manager init success, try_time=0\n"
     ]
    }
   ],
   "source": [
    "### Main code object\n",
    "\n",
    "leaderboard_evaluator = LeaderboardEvaluator(args, statistics_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.routes /mnt3/Documents/AD_Framework/Bench2Drive/leaderboard/data/bench2drive220.xml\n",
      "args.repetitions 1\n",
      "args.routes_subset 0\n"
     ]
    }
   ],
   "source": [
    "print(\"args.routes\", args.routes)\n",
    "print(\"args.repetitions\", args.repetitions)\n",
    "print(\"args.routes_subset\", args.routes_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_indexer = RouteIndexer(args.routes, args.repetitions, args.routes_subset)\n",
    "if args.resume:\n",
    "    resume = route_indexer.validate_and_resume(args.checkpoint)\n",
    "else:\n",
    "    resume = False\n",
    "\n",
    "if resume:\n",
    "    leaderboard_evaluator.statistics_manager.add_file_records(args.checkpoint)\n",
    "else:\n",
    "    leaderboard_evaluator.statistics_manager.clear_records()\n",
    "leaderboard_evaluator.statistics_manager.save_progress(route_indexer.index, route_indexer.total)\n",
    "leaderboard_evaluator.statistics_manager.write_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashed_flag = False\n",
    "t1 = time.time()\n",
    "route_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route_id:  1711\n",
      "route_id:  1773\n",
      "route_id:  1790\n",
      "Got the route_id:  1790\n",
      "\n",
      "=== Processing Route 1/220 ===\n",
      "Route: RouteScenario_1790, Town: Town12 Route_index:  3\n"
     ]
    }
   ],
   "source": [
    "### in the loop of running\n",
    "\n",
    "want_list = ['1790'] #['25845']\n",
    "\n",
    "for i in range(250):\n",
    "    config = route_indexer.get_next_config()\n",
    "    if config is None:\n",
    "        print(\"No more routes to process\")\n",
    "        \n",
    "\n",
    "    route_id = config.name.split(\"_\")[1]\n",
    "    print(\"route_id: \", route_id)\n",
    "    if route_id in want_list and len(want_list) > 0:\n",
    "        print(\"Got the route_id: \", route_id)\n",
    "        break\n",
    "    elif len(want_list) == 0:\n",
    "        route_id = route_id\n",
    "        print(\"Got the route_id: \", route_id)\n",
    "        break\n",
    "    \n",
    "print(f\"\\n=== Processing Route {route_count + 1}/{route_indexer.total} ===\")\n",
    "print(f\"Route: {config.name}, Town: {config.town}\", \"Route_index: \", route_indexer.index)\n",
    "\n",
    "### want route_id = 2509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m========= Preparing RouteScenario_1790 (repetition 0) =========\u001b[0m\n",
      "\u001b[1m> Loading the world\u001b[0m\n",
      "crash_flag:  False\n",
      "save_name:  RouteScenario_1790_rep0_Town12_HazardAtSideLane_1_8_08_29_22_31_46\n",
      "entry_status:  Started\n",
      "crash_message:  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load scenario\n",
    "\n",
    "crash_flag, save_name, entry_status, crash_message = my_load_scenario(leaderboard_evaluator, args, config)\n",
    "print(\"crash_flag: \", crash_flag)\n",
    "print(\"save_name: \", save_name)\n",
    "print(\"entry_status: \", entry_status)\n",
    "print(\"crash_message: \", crash_message)\n",
    "\n",
    "save_name = save_name + '_' + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Setting up the agent\u001b[0m\n",
      "DummyAgent2 initialized - will save 10 images to /mnt3/Documents/AD_Framework/town_test_images\n",
      "crash_flag:  False\n",
      "entry_status:  Started\n",
      "crash_message:  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load agent\n",
    "if not crash_flag:\n",
    "    crash_flag, entry_status, crash_message = my_load_agent(leaderboard_evaluator, args, config, save_name, entry_status, crash_message)\n",
    "\n",
    "print(\"crash_flag: \", crash_flag)\n",
    "print(\"entry_status: \", entry_status)\n",
    "print(\"crash_message: \", crash_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Running the route\u001b[0m\n",
      "crash_flag:  False\n",
      "entry_status:  Started\n",
      "crash_message:  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load route scenario\n",
    "if not crash_flag:\n",
    "    crash_flag, entry_status, crash_message = my_load_route_scenario(leaderboard_evaluator, args, config, entry_status, crash_message)\n",
    "\n",
    "print(\"crash_flag: \", crash_flag)\n",
    "print(\"entry_status: \", entry_status)\n",
    "print(\"crash_message: \", crash_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent enhanced with image capture capability\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Setup scenario\n",
    "if not crash_flag:\n",
    "    my_run_scenario_setup(leaderboard_evaluator)\n",
    "\n",
    "# Add image saving capability to the agent\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Monkey-patch the agent to capture sensor data\n",
    "original_run_step = leaderboard_evaluator.agent_instance.run_step\n",
    "\n",
    "def enhanced_run_step(input_data, timestamp):\n",
    "    \"\"\"Enhanced run_step that captures sensor data for image saving\"\"\"\n",
    "    # Store the input data for later access\n",
    "    leaderboard_evaluator.agent_instance.last_input_data = input_data\n",
    "    leaderboard_evaluator.agent_instance.step = getattr(leaderboard_evaluator.agent_instance, 'step', 0) + 1\n",
    "    \n",
    "    # Call original run_step\n",
    "    return original_run_step(input_data, timestamp)\n",
    "\n",
    "# Replace the run_step method\n",
    "leaderboard_evaluator.agent_instance.run_step = enhanced_run_step\n",
    "leaderboard_evaluator.agent_instance.step = 0\n",
    "\n",
    "print(\"Agent enhanced with image capture capability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images will be saved to: /mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices/notebooks/debug_snapshots/debug_images_20250829_224746\n",
      "\n",
      "=== Running 1 simulation steps with image capture ===\n",
      "In my_run_scenario_step (Check if it is stuck or not)\n",
      "=== [Agent] -- Wallclock = 2025-08-29 22:47:46.050 -- System time = 907.450 -- Game time = 2.650 -- Ratio = 0.003x\n",
      "Frame 53/10 - Test_0\n",
      "\n",
      "Step 0:\n",
      "\n",
      "=== Completed 1 steps ===\n",
      "crash_flag: False\n",
      "entry_status: Started\n",
      "crash_message: \n",
      "Images saved to: /mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices/notebooks/debug_snapshots/debug_images_20250829_224746\n",
      "\n",
      "Saved 1 steps of data\n",
      "  step_0000: 3 images\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Run scenario loop step by step with automatic image saving\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create save directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = Path(f\"/mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices/notebooks/debug_snapshots/debug_images_{timestamp}\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Images will be saved to: {save_dir}\")\n",
    "\n",
    "def save_debug_images(agent_instance, save_dir, step_count):\n",
    "    \"\"\"Save all sensor images for debugging\"\"\"\n",
    "    if not hasattr(agent_instance, 'last_input_data'):\n",
    "        return\n",
    "    \n",
    "    input_data = agent_instance.last_input_data\n",
    "    \n",
    "    # Create step-specific subdirectory\n",
    "    step_dir = save_dir / f\"step_{step_count:04d}\"\n",
    "    step_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save RGB cameras\n",
    "    for camera_id in ['Center', 'Left', 'Right']:\n",
    "        if camera_id in input_data:\n",
    "            rgb_image = input_data[camera_id][1][:, :, :3]  # Remove alpha channel if present\n",
    "            frame_num = input_data[camera_id][0]\n",
    "            \n",
    "            # Convert from RGB to BGR for OpenCV\n",
    "            bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Save image\n",
    "            image_path = step_dir / f\"{camera_id}_frame_{frame_num:06d}.jpg\"\n",
    "            cv2.imwrite(str(image_path), bgr_image)\n",
    "            # print(f\"  Saved: {camera_id} -> {image_path.name}\")\n",
    "    \n",
    "    # Save sensor data info to text file\n",
    "    info_path = step_dir / \"sensor_info.txt\"\n",
    "    with open(info_path, 'w') as f:\n",
    "        f.write(f\"Step: {step_count}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "        f.write(\"Sensor Data:\\n\")\n",
    "        for key, val in input_data.items():\n",
    "            if hasattr(val[1], 'shape'):\n",
    "                f.write(f\"  {key}: frame={val[0]:06d}, shape={val[1].shape}\\n\")\n",
    "            else:\n",
    "                f.write(f\"  {key}: frame={val[0]:06d}\\n\")\n",
    "    \n",
    "    return step_dir\n",
    "\n",
    "# Run simulation steps with image saving\n",
    "run_step_n = 1  # run 5 steps = 0.25 second\n",
    "total_steps_run = 0\n",
    "\n",
    "if not crash_flag:\n",
    "    print(f\"\\n=== Running {run_step_n} simulation steps with image capture ===\")\n",
    "    \n",
    "    for step_i in range(run_step_n):\n",
    "        # Run one step\n",
    "        crash_flag_step, entry_status, crash_message = my_run_scenario_step(\n",
    "            leaderboard_evaluator, entry_status, crash_message, n_steps=1\n",
    "        )\n",
    "        \n",
    "        if crash_flag_step:\n",
    "            crash_flag = True\n",
    "            print(f\"Crash detected at step {step_i}\")\n",
    "            break\n",
    "        \n",
    "        # Save images after each step\n",
    "        if hasattr(leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "            print(f\"\\nStep {total_steps_run + step_i}:\")\n",
    "            step_dir = save_debug_images(\n",
    "                leaderboard_evaluator.agent_instance, \n",
    "                save_dir, \n",
    "                total_steps_run + step_i\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Step {total_steps_run + step_i}: No sensor data captured\")\n",
    "    \n",
    "    total_steps_run += step_i + 1\n",
    "    \n",
    "print(f\"\\n=== Completed {total_steps_run} steps ===\")\n",
    "print(f\"crash_flag: {crash_flag}\")\n",
    "print(f\"entry_status: {entry_status}\")\n",
    "print(f\"crash_message: {crash_message}\")\n",
    "print(f\"Images saved to: {save_dir}\")\n",
    "\n",
    "# Display summary\n",
    "if save_dir.exists():\n",
    "    step_dirs = sorted(save_dir.glob(\"step_*\"))\n",
    "    print(f\"\\nSaved {len(step_dirs)} steps of data\")\n",
    "    for step_dir in step_dirs[:3]:  # Show first 3 steps\n",
    "        images = list(step_dir.glob(\"*.jpg\"))\n",
    "        print(f\"  {step_dir.name}: {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_obj = leaderboard_evaluator.manager.scenario.world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vehicles: 13\n",
      "3771 vehicle.lincoln.mkz_2020 Location(x=530.403687, y=3920.766357, z=371.202332)\n",
      "3770 vehicle.mini.cooper_s_2021 Location(x=530.563171, y=3917.517334, z=371.213531)\n",
      "3769 vehicle.ford.mustang Location(x=615.105164, y=3910.483154, z=371.281952)\n",
      "3768 vehicle.lincoln.mkz_2017 Location(x=598.434021, y=3910.554932, z=371.336395)\n",
      "3767 vehicle.dodge.charger_2020 Location(x=508.159088, y=3911.053955, z=371.199677)\n",
      "3766 vehicle.ford.mustang Location(x=495.933777, y=3911.112061, z=371.119751)\n",
      "3765 vehicle.ford.mustang Location(x=615.118042, y=3913.733398, z=371.281952)\n",
      "3764 vehicle.mercedes.coupe_2020 Location(x=597.304993, y=3913.803711, z=371.303741)\n",
      "3763 vehicle.chevrolet.impala Location(x=572.443298, y=3913.955322, z=371.291565)\n",
      "transform.location.x 572.4432983398438\n",
      "transform.location.y 3913.955322265625\n",
      "transform.location.z 371.29156494140625\n",
      "transform.rotation.yaw -179.70237731933594\n",
      "transform.rotation.pitch -0.05358966067433357\n",
      "transform.rotation.roll -0.0012207030085846782\n",
      "vel.x -5.230294704437256 vel.y -0.03011740930378437 vel.z -0.0054715536534786224\n",
      "3762 vehicle.dodge.charger_2020 Location(x=552.004456, y=3914.017334, z=371.266113)\n",
      "3697 vehicle.bh.crossbike Location(x=528.296570, y=3910.070557, z=371.202850)\n",
      "3696 vehicle.diamondback.century Location(x=568.289307, y=3914.000000, z=371.275269)\n",
      "3695 vehicle.lincoln.mkz_2020 Location(x=579.208984, y=3910.705322, z=371.297699)\n"
     ]
    }
   ],
   "source": [
    "actors = world_obj.get_actors()\n",
    "\n",
    "# Filter only vehicles\n",
    "vehicles = actors.filter(\"vehicle.*\")\n",
    "\n",
    "print(\"Total vehicles:\", len(vehicles))\n",
    "for v in vehicles:\n",
    "    print(v.id, v.type_id, v.get_location())\n",
    "\n",
    "    if str(v.id) == str(3763):\n",
    "        get_transform_info(v)\n",
    "        set_carla_loc_rot(v, 572.4432983398438, 3910.955322265625, 371.29156494140625, -179.70237731933594, -0.05358966067433357, -0.0012207030085846782)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<carla.libcarla.Vehicle at 0x7f0c658cf4a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3763 vehicle.chevrolet.impala Location(x=572.704773, y=3913.956787, z=371.291656)\n",
    "\n",
    "### Set position\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move with object without pause the running scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform.location.x 568.289306640625\n",
      "transform.location.y 3914.0\n",
      "transform.location.z 371.2752685546875\n",
      "transform.rotation.yaw 179.71975708007812\n",
      "transform.rotation.pitch -0.3596777319908142\n",
      "transform.rotation.roll 0.0013611284084618092\n",
      "vel.x -0.0007843188941478729 vel.y -7.83641153248027e-05 vel.z 0.2546674311161041\n",
      "----\n",
      "transform.location.x 528.2965698242188\n",
      "transform.location.y 3910.070556640625\n",
      "transform.location.z 371.2028503417969\n",
      "transform.rotation.yaw 179.72560119628906\n",
      "transform.rotation.pitch -0.21286283433437347\n",
      "transform.rotation.roll -9.155279258266091e-05\n",
      "vel.x -1.3914890587329865e-05 vel.y -0.00027825942379422486 vel.z 0.008414044976234436\n",
      "----\n",
      "transform.location.x 579.208984375\n",
      "transform.location.y 3910.705322265625\n",
      "transform.location.z 371.2976989746094\n",
      "transform.rotation.yaw 179.7611541748047\n",
      "transform.rotation.pitch 0.6786338686943054\n",
      "transform.rotation.roll 0.0014464346459135413\n",
      "vel.x -12.815984725952148 vel.y 0.05352751165628433 vel.z -0.01002237293869257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<carla.libcarla.Transform at 0x7f0c62779dc0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import carla\n",
    "\n",
    "def get_transform_info(carla_vehicle_object):\n",
    "    transform = carla_vehicle_object.get_transform()\n",
    "    print(\"transform.location.x\", transform.location.x)\n",
    "    print(\"transform.location.y\", transform.location.y)\n",
    "    print(\"transform.location.z\", transform.location.z)\n",
    "\n",
    "    print(\"transform.rotation.yaw\", transform.rotation.yaw)\n",
    "    print(\"transform.rotation.pitch\", transform.rotation.pitch)\n",
    "    print(\"transform.rotation.roll\", transform.rotation.roll)\n",
    "    vel = carla_vehicle_object.get_velocity()\n",
    "    print(\"vel.x\", vel.x, \"vel.y\", vel.y, \"vel.z\", vel.z)\n",
    "    return transform\n",
    "\n",
    "def set_carla_loc_rot(carla_vehicle_object, x, y, z, yaw, pitch, roll):\n",
    "    # Create a new location (x, y, z)\n",
    "    loc = carla.Location(x=x, y=y, z=z)\n",
    "    # Create a new rotation (pitch, yaw, roll)\n",
    "    rot = carla.Rotation(pitch=pitch, yaw=yaw, roll=roll)\n",
    "    new_transform = carla.Transform(loc, rot)\n",
    "    carla_vehicle_object.set_transform(new_transform)\n",
    "\n",
    "get_transform_info(leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0])\n",
    "print(\"----\")\n",
    "get_transform_info(leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[1])\n",
    "print(\"----\")\n",
    "get_transform_info(leaderboard_evaluator.manager.ego_vehicles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### move bike closer\n",
    "\n",
    "set_carla_loc_rot(carla_vehicle_object=leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0], x=568.29, y=3914, z=371.2144, yaw=179.72, pitch=-0.458, roll=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relocate my car\n",
    "\n",
    "set_carla_loc_rot(carla_vehicle_object=leaderboard_evaluator.manager.ego_vehicles[0], x=578.208984375, y=3910, z=371.29, yaw=179.76, pitch=0.708, roll=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaderboard_evaluator.manager._running True\n",
      "leaderboard_evaluator.manager._running False\n"
     ]
    }
   ],
   "source": [
    "### stop scenario manager\n",
    "\n",
    "# print(\"leaderboard_evaluator.manager._scenario_thread.__dict__._is_stopped\", leaderboard_evaluator.manager._scenario_thread._is_stopped)\n",
    "print(\"leaderboard_evaluator.manager._running\", leaderboard_evaluator.manager._running)\n",
    "leaderboard_evaluator.manager._running = False\n",
    "try:\n",
    "    leaderboard_evaluator.manager._scenario_thread.join()\n",
    "except:\n",
    "    pass\n",
    "leaderboard_evaluator.manager._scenario_thread = None\n",
    "# print(\"leaderboard_evaluator.manager._scenario_thread.__dict__._is_stopped\", leaderboard_evaluator.manager._scenario_thread._is_stopped)\n",
    "print(\"leaderboard_evaluator.manager._running\", leaderboard_evaluator.manager._running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start scenario manager\n",
    "\n",
    "# leaderboard_evaluator.manager._tick_scenario()\n",
    "leaderboard_evaluator.manager._running = True\n",
    "# CarlaDataProvider.on_carla_tick()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [Agent] -- Wallclock = 2025-08-29 22:34:15.245 -- System time = 96.645 -- Game time = 2.550 -- Ratio = 0.026x\n",
      "Frame 51/10 - Test_0\n"
     ]
    }
   ],
   "source": [
    "### continue the previous thread\n",
    "\n",
    "# def start_previous_loop_thread(leaderboard_evaluator_manager_obj):\n",
    "#     # Thread for build_scenarios\n",
    "#     leaderboard_evaluator_manager_obj._running = True\n",
    "#     leaderboard_evaluator_manager_obj._scenario_thread = threading.Thread(target=leaderboard_evaluator_manager_obj.build_scenarios_loop, args=(leaderboard_evaluator_manager_obj._debug_mode > 0, ))\n",
    "#     leaderboard_evaluator_manager_obj._scenario_thread.start()\n",
    "#     print(\"leaderboard_evaluator.manager._scenario_thread.is_alive()\", leaderboard_evaluator.manager._scenario_thread.is_alive())\n",
    "\n",
    "#     if leaderboard_evaluator_manager_obj._running:\n",
    "#         leaderboard_evaluator_manager_obj._tick_scenario()\n",
    "\n",
    "def start_builder_thread(manager):\n",
    "    if getattr(manager, \"_scenario_thread\", None) and manager._scenario_thread.is_alive():\n",
    "        return  # already running\n",
    "    manager._running = True\n",
    "    manager._scenario_thread = threading.Thread(\n",
    "        target=manager.build_scenarios_loop,\n",
    "        args=(manager._debug_mode > 0,),\n",
    "        daemon=True\n",
    "    )\n",
    "    manager._scenario_thread.start()\n",
    "\n",
    "    if manager._running:\n",
    "        manager._tick_scenario()\n",
    "\n",
    "# start_previous_loop_thread(leaderboard_evaluator.manager)\n",
    "start_builder_thread(leaderboard_evaluator.manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager._running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager._scenario_thread.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager._scenario_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.ego_vehicles[0].get_location().x, leaderboard_evaluator.manager.ego_vehicles[0].get_location().y, leaderboard_evaluator.manager.ego_vehicles[0].get_location().z\n",
    "\n",
    "# leaderboard_evaluator.manager.ego_vehicles[0].attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla import Location\n",
    "new_loc = Location(x=589.208984375, y=3910.0215, z=371.29)\n",
    "\n",
    "leaderboard_evaluator.manager.ego_vehicles[0].set_location(new_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.route_scenario.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.route_scenario.list_scenarios[0].other_actors[0].attributes\n",
    "# leaderboard_evaluator.route_scenario.occupied_parking_locations\n",
    "# leaderboard_evaluator.route_scenario.available_parking_locations\n",
    "# leaderboard_evaluator.route_scenario._parked_ids\n",
    "# leaderboard_evaluator.route_scenario.ego_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.route_scenario.list_scenarios[0].other_actors[1].get_location().x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0].get_location().x, leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0].get_location().y, leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0].get_location().z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla import Location\n",
    "new_loc = Location(x=568.2979, y=3914.0215, z=371.2144)\n",
    "\n",
    "leaderboard_evaluator.manager.scenario.list_scenarios[0].other_actors[0].set_location(new_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.route_scenario.other_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.other_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.ego_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.build_scenarios(leaderboard_evaluator.manager.scenario.ego_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.remove_all_actors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.ego_vehicles[0].get_location().x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.build_scenarios_loop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.agent_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(leaderboard_evaluator, 'world'):\n",
    "    world = leaderboard_evaluator.world\n",
    "    vehicles = world.get_actors().filter('vehicle.*')\n",
    "\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v in vehicles:\n",
    "#     print(\"v\", v.attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.list_scenarios\n",
    "leaderboard_evaluator.manager.scenario.occupied_parking_locations\n",
    "leaderboard_evaluator.manager.scenario.available_parking_locations\n",
    "leaderboard_evaluator.manager.scenario._parked_ids\n",
    "leaderboard_evaluator.manager.scenario.ego_data\n",
    "leaderboard_evaluator.manager.scenario.behavior_node\n",
    "leaderboard_evaluator.manager.scenario.other_actors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.manager.scenario.list_scenarios[0].remove_all_actors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_evaluator.route_scenario.other_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for snapshot functionality\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Add the parent directory to path to import from carla_server\n",
    "carla_server_path = '/mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices'\n",
    "if carla_server_path not in sys.path:\n",
    "    sys.path.insert(0, carla_server_path)\n",
    "\n",
    "# Import the WorldSnapshot class\n",
    "from world_snapshot import WorldSnapshot\n",
    "\n",
    "# Create a simple simulation state object to hold our snapshot\n",
    "class SimState:\n",
    "    def __init__(self):\n",
    "        self.leaderboard_evaluator = None\n",
    "        self.snapshots = {}\n",
    "        self.snapshot_dir = Path(\"/mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices/notebooks/debug_snapshots\")\n",
    "        self.snapshot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.step_count = 0\n",
    "        self.server_id = \"notebook_test\"\n",
    "        self.last_observation = None\n",
    "\n",
    "# Initialize simulation state\n",
    "sim_state = SimState()\n",
    "sim_state.leaderboard_evaluator = leaderboard_evaluator\n",
    "sim_state.step_count = total_steps_run\n",
    "\n",
    "print(f\"Initialized sim_state with leaderboard_evaluator\")\n",
    "print(f\"Snapshot directory: {sim_state.snapshot_dir}\")\n",
    "print(f\"Current step count: {sim_state.step_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a snapshot (adapted from carla_server.py)\n",
    "def save_snapshot(snapshot_id=None):\n",
    "    \"\"\"Save current world state\"\"\"\n",
    "    try:\n",
    "        # Verify scenario manager exists\n",
    "        if not hasattr(sim_state.leaderboard_evaluator, 'manager') or sim_state.leaderboard_evaluator.manager is None:\n",
    "            raise ValueError(\"Scenario manager not initialized - cannot snapshot without manager\")\n",
    "        \n",
    "        # Generate snapshot ID if not provided\n",
    "        if snapshot_id is None:\n",
    "            snapshot_id = f\"snap_{sim_state.server_id}_{len(sim_state.snapshots)}_{datetime.now().strftime('%H%M%S')}\"\n",
    "        \n",
    "        # Get world from leaderboard_evaluator\n",
    "        world = None\n",
    "        if hasattr(sim_state.leaderboard_evaluator, 'world'):\n",
    "            world = sim_state.leaderboard_evaluator.world\n",
    "            print(f\"DEBUG: Got world from leaderboard_evaluator: {world}\")\n",
    "        else:\n",
    "            print(\"ERROR: leaderboard_evaluator has no world attribute!\")\n",
    "        \n",
    "        # Debug: Print available attributes\n",
    "        print(\"DEBUG: leaderboard_evaluator attributes:\")\n",
    "        print(f\"  - Has manager: {hasattr(sim_state.leaderboard_evaluator, 'manager')}\")\n",
    "        print(f\"  - Has world: {hasattr(sim_state.leaderboard_evaluator, 'world')}\")\n",
    "        print(f\"  - World object: {world}\")\n",
    "        print(f\"  - Has agent_instance: {hasattr(sim_state.leaderboard_evaluator, 'agent_instance')}\")\n",
    "        print(f\"  - Has route_scenario: {hasattr(sim_state.leaderboard_evaluator, 'route_scenario')}\")\n",
    "        \n",
    "        # Get current observation if available\n",
    "        current_observation = {}\n",
    "        if hasattr(sim_state.leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "            input_data = sim_state.leaderboard_evaluator.agent_instance.last_input_data\n",
    "            current_observation = {\n",
    "                'images': {},\n",
    "                'sensors': {}\n",
    "            }\n",
    "            # Store sensor data\n",
    "            for key, val in input_data.items():\n",
    "                if key in ['Center', 'Left', 'Right']:\n",
    "                    current_observation['images'][key] = val[1]  # Store the image array\n",
    "                else:\n",
    "                    current_observation['sensors'][key] = val\n",
    "            print(f\"Captured observation with {len(current_observation['images'])} images\")\n",
    "        \n",
    "        # Capture the snapshot\n",
    "        phase_marker = f\"Step {sim_state.step_count} - {snapshot_id}\"\n",
    "        snapshot = WorldSnapshot.capture(sim_state, world, phase_marker=phase_marker)\n",
    "        snapshot.snapshot_id = snapshot_id\n",
    "        \n",
    "        # Store observation in snapshot\n",
    "        snapshot.observation = current_observation\n",
    "        \n",
    "        # Store a deep copy of the snapshot\n",
    "        sim_state.snapshots[snapshot_id] = copy.deepcopy(snapshot)\n",
    "        \n",
    "        # Save snapshot to disk\n",
    "        snapshot_file = sim_state.snapshot_dir / f\"{snapshot_id}.pkl\"\n",
    "        try:\n",
    "            with open(snapshot_file, 'wb') as f:\n",
    "                pickle.dump(snapshot, f)\n",
    "            print(f\"Snapshot saved to disk: {snapshot_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save snapshot to disk: {e}\")\n",
    "        \n",
    "        # Print snapshot details\n",
    "        print(f\"\\\\nSnapshot captured: {snapshot_id}\")\n",
    "        print(f\"  - Vehicles: {len(snapshot.vehicles)}\")\n",
    "        print(f\"  - Has scenario_manager state: {snapshot.scenario_manager is not None}\")\n",
    "        print(f\"  - Saved to: {snapshot_file}\")\n",
    "        \n",
    "        # Log ego vehicle position\n",
    "        for vehicle_id, vehicle_state in snapshot.vehicles.items():\n",
    "            if vehicle_state.is_hero:\n",
    "                print(f\"  - Ego position: x={vehicle_state.location['x']:.2f}, y={vehicle_state.location['y']:.2f}, z={vehicle_state.location['z']:.2f}\")\n",
    "                break\n",
    "        \n",
    "        if snapshot.scenario_manager:\n",
    "            print(f\"  - Manager tick: {snapshot.scenario_manager.tick_count}\")\n",
    "            print(f\"  - Manager running: {snapshot.scenario_manager.running}\")\n",
    "            print(f\"  - Ego vehicles: {len(snapshot.scenario_manager.ego_vehicle_ids)}\")\n",
    "        \n",
    "        return snapshot_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving snapshot: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Save a snapshot of the current state\n",
    "print(\"\\\\n=== Saving Snapshot of Current State ===\")\n",
    "snapshot_id = save_snapshot(\"test_snapshot_after_50_steps\")\n",
    "\n",
    "if snapshot_id:\n",
    "    print(f\"\\\\n✅ Successfully saved snapshot: {snapshot_id}\")\n",
    "    print(f\"Total snapshots in memory: {len(sim_state.snapshots)}\")\n",
    "else:\n",
    "    print(\"\\\\n❌ Failed to save snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run for another 50 steps.\n",
    "\n",
    "# Run another 50 simulation steps with image saving\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running Another 50 Steps with Image Capture\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create new directory for this batch of images\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir_batch2 = Path(f\"/mnt3/Documents/AD_Framework/bench2drive-gymnasium/bench2drive_microservices/notebooks/debug_snapshots/debug_images_batch2_{timestamp}\")\n",
    "save_dir_batch2.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Images will be saved to: {save_dir_batch2}\")\n",
    "\n",
    "run_step_n = 50  # Run 50 more steps\n",
    "initial_step_count = total_steps_run\n",
    "\n",
    "if not crash_flag:\n",
    "    print(f\"\\n=== Running {run_step_n} simulation steps with image capture ===\")\n",
    "    print(f\"Starting from step {total_steps_run}\")\n",
    "    \n",
    "    for step_i in range(run_step_n):\n",
    "        # Run one step\n",
    "        crash_flag_step, entry_status, crash_message = my_run_scenario_step(\n",
    "            leaderboard_evaluator, entry_status, crash_message, n_steps=1\n",
    "        )\n",
    "        \n",
    "        if crash_flag_step:\n",
    "            crash_flag = True\n",
    "            print(f\"Crash detected at step {step_i}\")\n",
    "            break\n",
    "        \n",
    "        # Save images after each step\n",
    "        if hasattr(leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "            # Save images every 1 steps to reduce storage\n",
    "            if step_i % 1 == 0:\n",
    "                print(f\"\\nStep {total_steps_run + step_i}:\")\n",
    "                step_dir = save_debug_images(\n",
    "                    leaderboard_evaluator.agent_instance, \n",
    "                    save_dir_batch2, \n",
    "                    total_steps_run + step_i\n",
    "                )\n",
    "                \n",
    "                # Also log vehicle position\n",
    "                if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "                    vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "                    transform = vehicle.get_transform()\n",
    "                    print(f\"  Vehicle position: x={transform.location.x:.2f}, y={transform.location.y:.2f}, z={transform.location.z:.2f}\")\n",
    "        else:\n",
    "            if step_i % 10 == 0:\n",
    "                print(f\"Step {total_steps_run + step_i}: Running...\")\n",
    "    \n",
    "    total_steps_run += step_i + 1\n",
    "    \n",
    "print(f\"\\n=== Completed {step_i + 1} additional steps ===\")\n",
    "print(f\"Total steps run in session: {total_steps_run}\")\n",
    "print(f\"crash_flag: {crash_flag}\")\n",
    "print(f\"entry_status: {entry_status}\")\n",
    "print(f\"crash_message: {crash_message}\")\n",
    "print(f\"Images saved to: {save_dir_batch2}\")\n",
    "\n",
    "# Display summary\n",
    "if save_dir_batch2.exists():\n",
    "    step_dirs = sorted(save_dir_batch2.glob(\"step_*\"))\n",
    "    print(f\"\\nSaved {len(step_dirs)} sets of images\")\n",
    "    for step_dir in step_dirs[:3]:  # Show first 3 steps\n",
    "        images = list(step_dir.glob(\"*.jpg\"))\n",
    "        print(f\"  {step_dir.name}: {len(images)} images\")\n",
    "\n",
    "# Save a snapshot after these 50 steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving Snapshot After Additional 50 Steps\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "snapshot_after_100 = save_snapshot(f\"snapshot_after_{total_steps_run}_steps\")\n",
    "if snapshot_after_100:\n",
    "    print(f\"✅ Saved snapshot: {snapshot_after_100}\")\n",
    "    print(f\"Total snapshots available: {len(sim_state.snapshots)}\")\n",
    "    \n",
    "    # List all snapshots\n",
    "    print(\"\\nAll available snapshots:\")\n",
    "    for snap_id in sim_state.snapshots.keys():\n",
    "        print(f\"  - {snap_id}\")\n",
    "\n",
    "# Update sim_state step count\n",
    "sim_state.step_count = total_steps_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restore snapshot \n",
    "\n",
    "# Restore to a previous snapshot and verify the state\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Restoring to Previous Snapshot\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display all available snapshots\n",
    "print(\"\\nAvailable snapshots to restore from:\")\n",
    "for i, snap_id in enumerate(sim_state.snapshots.keys()):\n",
    "    snapshot = sim_state.snapshots[snap_id]\n",
    "    print(f\"  {i+1}. {snap_id}\")\n",
    "    print(f\"     - Vehicles: {len(snapshot.vehicles)}\")\n",
    "    if hasattr(snapshot, 'scenario_manager') and snapshot.scenario_manager:\n",
    "        print(f\"     - Manager tick: {snapshot.scenario_manager.tick_count}\")\n",
    "    # Show ego position in snapshot  \n",
    "    for vehicle_id, vehicle_state in snapshot.vehicles.items():\n",
    "        if vehicle_state.is_hero:\n",
    "            print(f\"     - Ego position: x={vehicle_state.location['x']:.2f}, y={vehicle_state.location['y']:.2f}\")\n",
    "            break\n",
    "\n",
    "# Choose which snapshot to restore (you can change this)\n",
    "snapshot_to_restore = \"test_snapshot_after_50_steps\"  # Change this to any snapshot ID you want\n",
    "\n",
    "print(f\"\\n>>> Restoring to: {snapshot_to_restore}\")\n",
    "\n",
    "# Get current state BEFORE restore\n",
    "print(\"\\n=== Current State Before Restore ===\")\n",
    "current_position = None\n",
    "current_tick_count = None\n",
    "if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "    vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "    transform = vehicle.get_transform()\n",
    "    current_position = (transform.location.x, transform.location.y, transform.location.z)\n",
    "    print(f\"Current position: x={current_position[0]:.2f}, y={current_position[1]:.2f}, z={current_position[2]:.2f}\")\n",
    "\n",
    "if hasattr(leaderboard_evaluator, 'manager') and leaderboard_evaluator.manager:\n",
    "    current_tick_count = leaderboard_evaluator.manager.tick_count\n",
    "    print(f\"Current manager tick count: {current_tick_count}\")\n",
    "    print(f\"Current total_steps_run variable: {total_steps_run}\")\n",
    "\n",
    "# Save images BEFORE restore for comparison\n",
    "print(\"\\n=== Saving Images Before Restore ===\")\n",
    "timestamp_before = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "before_restore_dir = sim_state.snapshot_dir / f\"before_restore_{timestamp_before}\"\n",
    "before_restore_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if hasattr(leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "    save_debug_images(leaderboard_evaluator.agent_instance, before_restore_dir, current_tick_count if current_tick_count else total_steps_run)\n",
    "    print(f\"Saved 'before' images to: {before_restore_dir}\")\n",
    "\n",
    "# Perform the restore\n",
    "try:\n",
    "    print(f\"\\n=== Starting Restore of Snapshot: {snapshot_to_restore} ===\")\n",
    "    \n",
    "    # Get the snapshot\n",
    "    if snapshot_to_restore not in sim_state.snapshots:\n",
    "        # Try to load from disk if not in memory\n",
    "        snapshot_file = sim_state.snapshot_dir / f\"{snapshot_to_restore}.pkl\"\n",
    "        if snapshot_file.exists():\n",
    "            print(f\"Loading snapshot from disk: {snapshot_file}\")\n",
    "            with open(snapshot_file, 'rb') as f:\n",
    "                snapshot = pickle.load(f)\n",
    "                sim_state.snapshots[snapshot_to_restore] = snapshot\n",
    "        else:\n",
    "            raise ValueError(f\"Snapshot not found: {snapshot_to_restore}\")\n",
    "    \n",
    "    snapshot = sim_state.snapshots[snapshot_to_restore]\n",
    "    print(f\"Found snapshot with {len(snapshot.vehicles)} vehicles\")\n",
    "    \n",
    "    # Get the world\n",
    "    world = sim_state.leaderboard_evaluator.world\n",
    "    if world is None:\n",
    "        raise ValueError(\"World is not available\")\n",
    "    \n",
    "    # CRITICAL FIX: The restore method signature is def restore(self, sim_state, world)\n",
    "    print(\"\\nRestoring world state...\")\n",
    "    print(f\"DEBUG: Calling restore with sim_state={sim_state}, world={world}\")\n",
    "    \n",
    "    # Call restore with correct parameter order\n",
    "    snapshot.restore(sim_state, world)\n",
    "    \n",
    "    # CRITICAL: Force the manager tick count to be restored\n",
    "    if hasattr(snapshot, 'scenario_manager') and snapshot.scenario_manager:\n",
    "        if hasattr(leaderboard_evaluator, 'manager') and leaderboard_evaluator.manager:\n",
    "            restored_tick = snapshot.scenario_manager.tick_count\n",
    "            leaderboard_evaluator.manager.tick_count = restored_tick\n",
    "            print(f\"\\nForced manager tick count to: {restored_tick}\")\n",
    "            \n",
    "            # Also update our tracking variables to match\n",
    "            total_steps_run = restored_tick\n",
    "            sim_state.step_count = restored_tick\n",
    "            print(f\"Updated total_steps_run to: {total_steps_run}\")\n",
    "    \n",
    "    # CRITICAL FIX: Reinitialize watchdogs IMMEDIATELY after restore\n",
    "    print(\"\\n=== Fixing Watchdog Issues After Restore ===\")\n",
    "    from srunner.scenariomanager.watchdog import Watchdog\n",
    "    \n",
    "    if hasattr(leaderboard_evaluator, 'manager') and leaderboard_evaluator.manager:\n",
    "        # Fix manager watchdog\n",
    "        if not hasattr(leaderboard_evaluator.manager, '_watchdog') or leaderboard_evaluator.manager._watchdog is None:\n",
    "            print(\"Reinitializing manager watchdog...\")\n",
    "            leaderboard_evaluator.manager._watchdog = Watchdog(leaderboard_evaluator.manager._timeout)\n",
    "            leaderboard_evaluator.manager._watchdog.start()\n",
    "            print(\"✅ Manager watchdog reinitialized\")\n",
    "        \n",
    "        # Fix agent watchdog\n",
    "        if not hasattr(leaderboard_evaluator.manager, '_agent_watchdog') or leaderboard_evaluator.manager._agent_watchdog is None:\n",
    "            print(\"Reinitializing agent watchdog...\")\n",
    "            leaderboard_evaluator.manager._agent_watchdog = Watchdog(leaderboard_evaluator.manager._timeout)\n",
    "            leaderboard_evaluator.manager._agent_watchdog.start()\n",
    "            print(\"✅ Agent watchdog reinitialized\")\n",
    "    \n",
    "    # Force a world tick to apply restored positions\n",
    "    print(\"\\nForcing world tick to apply restored positions...\")\n",
    "    if hasattr(leaderboard_evaluator, 'world') and leaderboard_evaluator.world:\n",
    "        try:\n",
    "            leaderboard_evaluator.world.tick()\n",
    "            print(\"✅ World ticked successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to tick world: {e}\")\n",
    "    \n",
    "    # Verify ego position after restore\n",
    "    print(\"\\n=== State After Restore ===\")\n",
    "    if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "        vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "        transform = vehicle.get_transform()\n",
    "        restored_position = (transform.location.x, transform.location.y, transform.location.z)\n",
    "        print(f\"Position AFTER restore: x={restored_position[0]:.2f}, y={restored_position[1]:.2f}, z={restored_position[2]:.2f}\")\n",
    "        \n",
    "        # Calculate distance moved\n",
    "        if current_position:\n",
    "            distance = ((restored_position[0] - current_position[0])**2 + \n",
    "                       (restored_position[1] - current_position[1])**2 + \n",
    "                       (restored_position[2] - current_position[2])**2)**0.5\n",
    "            print(f\"Vehicle moved {distance:.2f} units during restore\")\n",
    "    \n",
    "    if hasattr(leaderboard_evaluator, 'manager') and leaderboard_evaluator.manager:\n",
    "        print(f\"Manager tick count after restore: {leaderboard_evaluator.manager.tick_count}\")\n",
    "    \n",
    "    # Restore observation if available and save restored images\n",
    "    if hasattr(snapshot, 'observation') and snapshot.observation:\n",
    "        sim_state.last_observation = snapshot.observation\n",
    "        print(f\"\\nRestored observation with {len(snapshot.observation.get('images', {}))} images\")\n",
    "        \n",
    "        # Save the restored images\n",
    "        print(\"\\n=== Saving Restored Images ===\")\n",
    "        timestamp_after = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        after_restore_dir = sim_state.snapshot_dir / f\"after_restore_{timestamp_after}\"\n",
    "        after_restore_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save each camera image from the restored observation\n",
    "        for camera_id in ['Center', 'Left', 'Right']:\n",
    "            if camera_id in snapshot.observation.get('images', {}):\n",
    "                img_array = snapshot.observation['images'][camera_id]\n",
    "                if len(img_array.shape) == 3:  # Make sure it's an image\n",
    "                    # Convert and save\n",
    "                    bgr_image = cv2.cvtColor(img_array[:, :, :3], cv2.COLOR_RGB2BGR)\n",
    "                    image_path = after_restore_dir / f\"restored_{camera_id}.jpg\"\n",
    "                    cv2.imwrite(str(image_path), bgr_image)\n",
    "                    print(f\"  Saved restored {camera_id} image to {image_path.name}\")\n",
    "        \n",
    "        print(f\"Restored images saved to: {after_restore_dir}\")\n",
    "    \n",
    "    print(f\"\\n✅ Successfully restored snapshot: {snapshot_to_restore}\")\n",
    "    print(f\"   Simulation state has been reset to tick {total_steps_run}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error restoring snapshot: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Run 50 steps after restore to fully demonstrate it's working\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running 50 Steps After Restore with Image Capture\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timestamp_verify = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "verify_dir = sim_state.snapshot_dir / f\"verify_after_restore_50steps_{timestamp_verify}\"\n",
    "verify_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "verification_steps = 50  # Run 50 steps to show proper restoration\n",
    "\n",
    "# CRITICAL: Use the restored tick count as the starting point\n",
    "restored_step_start = leaderboard_evaluator.manager.tick_count if hasattr(leaderboard_evaluator, 'manager') else total_steps_run\n",
    "\n",
    "print(f\"Starting from restored tick: {restored_step_start}\")\n",
    "print(f\"Will run {verification_steps} steps and save images every step\")\n",
    "print(f\"Expected to reach tick: {restored_step_start + verification_steps}\\n\")\n",
    "\n",
    "for step_i in range(verification_steps):\n",
    "    # Run one step\n",
    "    crash_flag_step, entry_status, crash_message = my_run_scenario_step(\n",
    "        leaderboard_evaluator, entry_status, crash_message, n_steps=1\n",
    "    )\n",
    "    \n",
    "    if crash_flag_step:\n",
    "        print(f\"Crash detected at verification step {step_i}\")\n",
    "        break\n",
    "    \n",
    "    # Get current tick from manager\n",
    "    current_tick = leaderboard_evaluator.manager.tick_count if hasattr(leaderboard_evaluator, 'manager') else (restored_step_start + step_i)\n",
    "    \n",
    "    # Save images every step to show progression\n",
    "    if hasattr(leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "        print(f\"\\nTick {current_tick} (Verification step {step_i}):\")\n",
    "        step_dir = save_debug_images(\n",
    "            leaderboard_evaluator.agent_instance, \n",
    "            verify_dir, \n",
    "            current_tick  # Use actual tick count, not step index\n",
    "        )\n",
    "        \n",
    "        # Also log vehicle position\n",
    "        if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "            vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "            transform = vehicle.get_transform()\n",
    "            print(f\"  Vehicle position: x={transform.location.x:.2f}, y={transform.location.y:.2f}, z={transform.location.z:.2f}\")\n",
    "    else:\n",
    "        # Show progress\n",
    "        if step_i % 10 == 0:\n",
    "            print(f\"Running tick {current_tick} (Verification step {step_i})...\")\n",
    "\n",
    "# Update total steps based on actual manager state\n",
    "if hasattr(leaderboard_evaluator, 'manager'):\n",
    "    total_steps_run = leaderboard_evaluator.manager.tick_count\n",
    "else:\n",
    "    total_steps_run = restored_step_start + step_i + 1\n",
    "\n",
    "# Final position and summary\n",
    "print(f\"\\n=== Completed {step_i + 1} verification steps ===\")\n",
    "print(f\"Started from tick: {restored_step_start}\")\n",
    "print(f\"Ended at tick: {total_steps_run}\")\n",
    "print(f\"Total verification steps run: {step_i + 1}\")\n",
    "\n",
    "if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "    vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "    transform = vehicle.get_transform()\n",
    "    print(f\"\\nFinal position after {verification_steps} steps: x={transform.location.x:.2f}, y={transform.location.y:.2f}, z={transform.location.z:.2f}\")\n",
    "    \n",
    "    # Compare with original snapshot position\n",
    "    for vehicle_id, vehicle_state in snapshot.vehicles.items():\n",
    "        if vehicle_state.is_hero:\n",
    "            original_x = vehicle_state.location['x']\n",
    "            original_y = vehicle_state.location['y']\n",
    "            distance_traveled = ((transform.location.x - original_x)**2 + \n",
    "                               (transform.location.y - original_y)**2)**0.5\n",
    "            print(f\"Distance traveled from restored point: {distance_traveled:.2f} units\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n✅ Successfully ran {step_i + 1} steps after restore!\")\n",
    "print(f\"Verification images saved to: {verify_dir}\")\n",
    "\n",
    "# Count saved images and check their names\n",
    "if verify_dir.exists():\n",
    "    step_dirs = sorted(verify_dir.glob(\"step_*\"))\n",
    "    print(f\"Saved {len(step_dirs)} sets of images\")\n",
    "    print(\"Image directories created:\")\n",
    "    for step_dir in step_dirs[:5]:  # Show first 5\n",
    "        print(f\"  - {step_dir.name}\")\n",
    "    if len(step_dirs) > 5:\n",
    "        print(f\"  ... and {len(step_dirs) - 5} more\")\n",
    "\n",
    "# Summary of all saved images\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESTORE VERIFICATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Before restore images: {before_restore_dir}\")\n",
    "print(f\"2. Restored snapshot images: {after_restore_dir if 'after_restore_dir' in locals() else 'Not saved (no observation in snapshot)'}\")\n",
    "print(f\"3. After restore verification (50 steps): {verify_dir}\")\n",
    "print(f\"\\nThe simulation was restored to tick {restored_step_start} and ran to tick {total_steps_run}\")\n",
    "print(f\"This confirms the restore {'WORKED - properly restored to step 50!' if restored_step_start == 50 else f'FAILED - started from tick {restored_step_start} instead of 50'}\")\n",
    "\n",
    "# CRITICAL: Check if the first image directory shows step 50 or 100\n",
    "if verify_dir.exists():\n",
    "    step_dirs = sorted(verify_dir.glob(\"step_*\"))\n",
    "    if step_dirs:\n",
    "        first_dir = step_dirs[0].name\n",
    "        print(f\"\\n🔍 CRITICAL CHECK - First verification image directory: {first_dir}\")\n",
    "        if \"step_0050\" in first_dir or \"step_0051\" in first_dir:\n",
    "            print(\"✅ Images show restore WORKED - continuing from step 50!\")\n",
    "        elif \"step_0100\" in first_dir or \"step_0101\" in first_dir:\n",
    "            print(\"❌ Images show restore FAILED - still at step 100!\")\n",
    "        else:\n",
    "            print(f\"⚠️ Unexpected step number in first directory: {first_dir}\")\n",
    "            \n",
    "print(\"\\n⚠️ IMPORTANT: Check the images in the verification directory to confirm:\")\n",
    "print(f\"   {verify_dir}\")\n",
    "print(\"   The images should show the scenery from step 50, NOT step 100!\")\n",
    "\n",
    "# Optional: Save a new snapshot after these 50 verification steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving New Snapshot After Verification\")\n",
    "print(\"=\"*60)\n",
    "sim_state.step_count = total_steps_run\n",
    "new_snapshot_id = save_snapshot(f\"snapshot_after_restore_verification_{total_steps_run}_steps\")\n",
    "if new_snapshot_id:\n",
    "    print(f\"✅ Saved new snapshot: {new_snapshot_id}\")\n",
    "    print(f\"Total snapshots available: {len(sim_state.snapshots)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix the restore by reinitializing watchdog and forcing tick\n",
    "\n",
    "# After restore, we need to fix the watchdog issue and force a proper world tick\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fixing Restore Issues\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fix 1: Reinitialize the watchdog that was set to None during restore\n",
    "if hasattr(leaderboard_evaluator, 'manager') and leaderboard_evaluator.manager:\n",
    "    from srunner.scenariomanager.watchdog import Watchdog\n",
    "    \n",
    "    # Recreate the watchdog if it's None\n",
    "    if not hasattr(leaderboard_evaluator.manager, '_watchdog') or leaderboard_evaluator.manager._watchdog is None:\n",
    "        print(\"Reinitializing manager watchdog...\")\n",
    "        leaderboard_evaluator.manager._watchdog = Watchdog(leaderboard_evaluator.manager._timeout)\n",
    "        leaderboard_evaluator.manager._watchdog.start()\n",
    "        print(\"✅ Watchdog reinitialized\")\n",
    "    \n",
    "    # Also fix agent watchdog if needed\n",
    "    if not hasattr(leaderboard_evaluator.manager, '_agent_watchdog') or leaderboard_evaluator.manager._agent_watchdog is None:\n",
    "        print(\"Reinitializing agent watchdog...\")\n",
    "        leaderboard_evaluator.manager._agent_watchdog = Watchdog(leaderboard_evaluator.manager._timeout)\n",
    "        leaderboard_evaluator.manager._agent_watchdog.start()\n",
    "        print(\"✅ Agent watchdog reinitialized\")\n",
    "\n",
    "# Fix 2: Force a world tick to apply the restored positions\n",
    "print(\"\\nForcing world tick to apply restored positions...\")\n",
    "if hasattr(leaderboard_evaluator, 'world') and leaderboard_evaluator.world:\n",
    "    try:\n",
    "        # Force synchronous tick\n",
    "        leaderboard_evaluator.world.tick()\n",
    "        print(\"✅ World ticked successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to tick world: {e}\")\n",
    "\n",
    "# Fix 3: Verify ego vehicle position after tick\n",
    "if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "    vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "    transform = vehicle.get_transform()\n",
    "    print(f\"\\nEgo position after fixes: x={transform.location.x:.2f}, y={transform.location.y:.2f}, z={transform.location.z:.2f}\")\n",
    "    \n",
    "    # Check if we're at the right position now\n",
    "    expected_x = 580.47  # From snapshot\n",
    "    expected_y = 3910.70\n",
    "    distance_from_expected = ((transform.location.x - expected_x)**2 + (transform.location.y - expected_y)**2)**0.5\n",
    "    if distance_from_expected < 5.0:  # Within 5 meters\n",
    "        print(f\"✅ Position restored correctly! (within {distance_from_expected:.2f}m of expected)\")\n",
    "    else:\n",
    "        print(f\"⚠️ Position still incorrect! {distance_from_expected:.2f}m from expected position\")\n",
    "        print(f\"   Expected: x={expected_x:.2f}, y={expected_y:.2f}\")\n",
    "        print(f\"   Got: x={transform.location.x:.2f}, y={transform.location.y:.2f}\")\n",
    "\n",
    "print(\"\\nRestore fixes applied. Ready to continue simulation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run verification steps after fixing the restore\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running Verification After Restore Fixes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timestamp_verify = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "verify_dir = sim_state.snapshot_dir / f\"verify_after_fix_{timestamp_verify}\"\n",
    "verify_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "verification_steps = 50\n",
    "restored_step_start = leaderboard_evaluator.manager.tick_count if hasattr(leaderboard_evaluator, 'manager') else 50\n",
    "\n",
    "print(f\"Starting from tick: {restored_step_start}\")\n",
    "print(f\"Will run {verification_steps} steps and save images every 5 steps\")\n",
    "print(f\"Images will be saved to: {verify_dir}\\n\")\n",
    "\n",
    "for step_i in range(verification_steps):\n",
    "    # Run one step\n",
    "    crash_flag_step, entry_status, crash_message = my_run_scenario_step(\n",
    "        leaderboard_evaluator, entry_status, crash_message, n_steps=1\n",
    "    )\n",
    "    \n",
    "    if crash_flag_step:\n",
    "        print(f\"Crash detected at verification step {step_i}\")\n",
    "        break\n",
    "    \n",
    "    # Get current tick from manager\n",
    "    current_tick = leaderboard_evaluator.manager.tick_count if hasattr(leaderboard_evaluator, 'manager') else (restored_step_start + step_i)\n",
    "    \n",
    "    # Save images every 5 steps\n",
    "    if step_i % 5 == 0:\n",
    "        if hasattr(leaderboard_evaluator.agent_instance, 'last_input_data'):\n",
    "            print(f\"\\nTick {current_tick} (Step {step_i}):\")\n",
    "            step_dir = save_debug_images(\n",
    "                leaderboard_evaluator.agent_instance, \n",
    "                verify_dir, \n",
    "                current_tick\n",
    "            )\n",
    "            \n",
    "            # Log vehicle position\n",
    "            if hasattr(leaderboard_evaluator.agent_instance, '_vehicle') and leaderboard_evaluator.agent_instance._vehicle:\n",
    "                vehicle = leaderboard_evaluator.agent_instance._vehicle\n",
    "                transform = vehicle.get_transform()\n",
    "                print(f\"  Vehicle at: x={transform.location.x:.2f}, y={transform.location.y:.2f}, z={transform.location.z:.2f}\")\n",
    "\n",
    "# Update total steps\n",
    "if hasattr(leaderboard_evaluator, 'manager'):\n",
    "    total_steps_run = leaderboard_evaluator.manager.tick_count\n",
    "else:\n",
    "    total_steps_run = restored_step_start + step_i + 1\n",
    "\n",
    "print(f\"\\n=== Verification Complete ===\")\n",
    "print(f\"Ran {step_i + 1} steps successfully\")\n",
    "print(f\"Final tick: {total_steps_run}\")\n",
    "print(f\"Images saved to: {verify_dir}\")\n",
    "\n",
    "# Check the saved images\n",
    "if verify_dir.exists():\n",
    "    step_dirs = sorted(verify_dir.glob(\"step_*\"))\n",
    "    print(f\"\\nSaved {len(step_dirs)} sets of images:\")\n",
    "    for step_dir in step_dirs[:3]:\n",
    "        print(f\"  - {step_dir.name}\")\n",
    "    \n",
    "    # CRITICAL: Check if the first image directory shows step 50 or 100\n",
    "    if step_dirs:\n",
    "        first_dir = step_dirs[0].name\n",
    "        print(f\"\\n🔍 First image directory: {first_dir}\")\n",
    "        if \"step_0050\" in first_dir or \"step_0051\" in first_dir:\n",
    "            print(\"✅ Images show restore WORKED - continuing from step 50!\")\n",
    "        elif \"step_0100\" in first_dir or \"step_0101\" in first_dir:\n",
    "            print(\"❌ Images show restore FAILED - still at step 100!\")\n",
    "        else:\n",
    "            print(f\"⚠️ Unexpected step number in first directory: {first_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Check these directories to verify the restore worked:\")\n",
    "print(f\"1. {verify_dir}/step_{restored_step_start:04d} - Should show scenery from step 50\")\n",
    "print(f\"2. Compare with original step 50 images\")\n",
    "print(\"\\nIf the images match the original step 50 scenery, the restore worked!\")\n",
    "print(\"If they show step 100 scenery, the restore failed to actually reset the world state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
